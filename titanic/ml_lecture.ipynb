{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7cb9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "boston_df = pd.read_csv('housing.csv')\n",
    "boston_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289cc99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(boston_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f39a26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 실제값 y, 예측값 y^ ​= w1​*RM + b 일 때\n",
    "\n",
    "def get_update_weights_value(bias, w1, w2, rm, lstat, target, learning_rate = 0.01):\n",
    "    N = target.shape[0]\n",
    "    predicted = (w1*rm) + (w2*lstat) + bias\n",
    "    diff = target - predicted\n",
    "    \n",
    "    w1_update = -(2/n) * learning_rate * (torch.matmul(rm,diff)) \n",
    "    # w1 미분 : RM * diff (w1을 1단위 바꾸면 RM이 지금의 오차 diff를 얼마나 크게 흔들까)\n",
    "    # RM이 큰 row는 w1를 살짝만 움직여도 예측깂 y^가 크게 움직임 -> diff (y-y^)도 크게 움직임 -> RM은 w1이 이 row에 얼마나 세게 작용하는지\n",
    "    # RM * w1은 w1 1단위 증가가 오차에 얼마나 기여하는지 계산. 이 row의 오차는 w1바뀌면 이만큼 반응해 -> row별 기여\n",
    "    # 기여도가 -면 over predict 의미 -> 다음 값에는 줄이자. learning_rate 곱한 값을 다음 값에 뺌 \n",
    "     \n",
    "\n",
    "    w2_update = -(2/n) * learning_rate * (torch.matmul(lstat, diff))\n",
    "    # matmul: rm[0]*diff[0] + rm[1]*diff[1] + ... = 모든 줄 오차 다 더한 숫자로\n",
    "\n",
    "    bias_update = -(2/n) * learning_rate * torch.sum(diff)\n",
    "    \n",
    "    mse_loss = torch.mean(diff **2) # 차이 제곱한걸 평균냄\n",
    "    \n",
    "    return bias_update, w1_update, w2_update, mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3caf19fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(features, target, iter_epochs = 1000, learing_rate = 0.01, verbose = 5000):\n",
    "    w1 = torch.zeros(size = (1,),dtype = torch.float32) # 초기화\n",
    "                                             # 가중치 값이 float로 더해지도록\n",
    "    w2 = torch.zeros(size = (1,),dtype = torch.float32)\n",
    "    bias = torch.zeros(size = (1,),dtype = torch.float32)\n",
    "    print(w1.item(), w2.item(), bias.item()) # 값 보기\n",
    "    \n",
    "rm = features[:,0] # 첫번째 칼럼\n",
    "lstat = features[:,1] \n",
    "    \n",
    "for i in range(1, iter_epochs+1):\n",
    "    bias_update, w1_update, w2_update, loss = get_update_weights_value(bias, weight1, weight2, rm, lstat, target, learning_rate)\n",
    "    w1 = w1 - w1_update\n",
    "    w2 = w2 - w2_update\n",
    "    bias = bias-bias_update\n",
    "    if verbose:\n",
    "        if i%10 == 0:\n",
    "            print(i/iter_epochs)\n",
    "            print(w1.item(), w2.item(), bias.item(), loss.item())\n",
    "return w1,w2,bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f7bb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 딥러닝 쓴 선형 회귀 시 scaling 시켜야 - RM, LSTAT 값의 범위가 서로 다르므로\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaled_features_np = scaler.fit_transform(boston_df[['RM', 'LSTAT']])\n",
    "\n",
    "scaled_features_ts = torch.from_numpy(scaled_features_np)\n",
    "target_ts = torch.from_numpy(boston_df['PRICE'].values)\n",
    "w1,w2,bias = gradient_descent(scaled_features_ts, target_ts, iter_epochs=5000, verbose= True)\n",
    "print(w1.item(), w2.item(), bias.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947459b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python_new_(.venv)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
